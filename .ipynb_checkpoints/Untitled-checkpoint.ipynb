{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "env = gym.make('MountainCar-v0').env\n",
    "env.reset()\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, 50)\n",
    "        self.linear2 = nn.Linear(50, 50)\n",
    "        self.head = nn.Linear(50, output_size)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE = 10\n",
    "INPUT_SIZE = 2\n",
    "OUTPUT_SIZE = 3\n",
    "LR = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_done = 0\n",
    "\n",
    "def eps_greedy_select_action(policy_net, state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(3)]], device=device, dtype=torch.long)\n",
    "    \n",
    "def select_action(policy_net, state):\n",
    "    with torch.no_grad():\n",
    "        return policy_net(state).max(1)[1].view(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model(optimizer, policy_net, target_net, memory, perturb=False):\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.uint8)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    rewards = list(batch.reward)\n",
    "    if perturb:\n",
    "        for i, _ in enumerate(rewards):\n",
    "            rewards[i] += np.random.normal(0, 1)\n",
    "\n",
    "    reward_batch = torch.cat(tuple(rewards))\n",
    "\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    \n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    loss_fn = nn.MSELoss()\n",
    "    loss = loss_fn(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "#     for param in policy_net.parameters():\n",
    "#         param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dac58b9db52451398137385e9718836",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cat(): argument 'tensors' (position 1) must be tuple of Tensors, not Tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-df5a5c959f19>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[0moptimize_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy_net\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_net\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mperturbed_memory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mperturb\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi_episode\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mTARGET_UPDATE\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-98b7258128b7>\u001b[0m in \u001b[0;36moptimize_model\u001b[1;34m(optimizer, policy_net, target_net, memory, perturb)\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0mrewards\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mreward_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mstate_action_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpolicy_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: cat(): argument 'tensors' (position 1) must be tuple of Tensors, not Tensor"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "policy_net = DQN(INPUT_SIZE, OUTPUT_SIZE).to(device)\n",
    "target_net = DQN(INPUT_SIZE, OUTPUT_SIZE).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.SGD(policy_net.parameters(), lr=LR)\n",
    "num_episodes = 2000\n",
    "cumulative_rewards_1 = [0 for _ in range(num_episodes)]\n",
    "perturbed_memory = ReplayMemory(100000)\n",
    "\n",
    "for i_episode in tqdm(range(num_episodes), ascii=True):\n",
    "    state = env.reset()\n",
    "    state = torch.tensor([state], device=device, dtype=torch.float)\n",
    "    for t in range(500):\n",
    "#         if (i_episode + 1) % 100 == 0:\n",
    "#             env.render()\n",
    "        action = select_action(policy_net, state)\n",
    "        next_state, reward, done, _ = env.step(action.item())\n",
    "        next_state = torch.tensor([next_state], device=device, dtype=torch.float)\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        if done:\n",
    "            next_state = None\n",
    "        \n",
    "        perturbed_memory.push(state, action, next_state, reward)\n",
    "        cumulative_rewards_1[i_episode] += reward.item()\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "        state = next_state\n",
    "        optimize_model(optimizer, policy_net, target_net, perturbed_memory, perturb=True)\n",
    "\n",
    "    if (i_episode + 1) % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "    if (i_episode + 1) % 100 == 0:\n",
    "        print('Episode ' + str(i_episode))\n",
    "        print('Cumulative reward: ' + str(cumulative_rewards_1[i_episode]))\n",
    "        \n",
    "print('Complete')\n",
    "plt.title('Training...')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Duration')\n",
    "plt.plot(cumulative_rewards_1)\n",
    "plt.show()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f922b0c7ae3c4d5ba2784b75bf889b7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 99\n",
      "Cumulative reward: -500.0\n",
      "Episode 199\n",
      "Cumulative reward: -500.0\n",
      "Episode 299\n",
      "Cumulative reward: -500.0\n",
      "Episode 399\n",
      "Cumulative reward: -500.0\n",
      "Episode 499\n",
      "Cumulative reward: -500.0\n",
      "Episode 599\n",
      "Cumulative reward: -500.0\n",
      "Episode 699\n",
      "Cumulative reward: -500.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-e5a393d3317d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[0moptimize_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy_net\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_net\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps_greedy_memory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi_episode\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mTARGET_UPDATE\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-2011067824f4>\u001b[0m in \u001b[0;36moptimize_model\u001b[1;34m(optimizer, policy_net, target_net, memory)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTransition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtransitions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n\u001b[1;32m----> 7\u001b[1;33m                                           batch.next_state)), device=device, dtype=torch.uint8)\n\u001b[0m\u001b[0;32m      8\u001b[0m     non_final_next_states = torch.cat([s for s in batch.next_state\n\u001b[0;32m      9\u001b[0m                                                 if s is not None])\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "policy_net = DQN(INPUT_SIZE, OUTPUT_SIZE).to(device)\n",
    "target_net = DQN(INPUT_SIZE, OUTPUT_SIZE).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.SGD(policy_net.parameters(), lr=LR)\n",
    "num_episodes = 2000\n",
    "cumulative_rewards_2 = [0 for _ in range(num_episodes)]\n",
    "eps_greedy_memory = ReplayMemory(100000)\n",
    "\n",
    "episode_steps = []\n",
    "for i_episode in tqdm(range(num_episodes), ascii=True):\n",
    "    state = env.reset()\n",
    "    state = torch.tensor([state], device=device, dtype=torch.float)\n",
    "    for t in range(500):\n",
    "#         if (i_episode + 1) % 100 == 0:\n",
    "#             env.render()\n",
    "        action = select_action(policy_net, state)\n",
    "        next_state, reward, done, _ = env.step(action.item())\n",
    "        next_state = torch.tensor([next_state], device=device, dtype=torch.float)\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        if done:\n",
    "            next_state = None\n",
    "        \n",
    "        eps_greedy_memory.push(state, action, next_state, reward)\n",
    "        cumulative_rewards_2[i_episode] += reward.item()\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "        state = next_state\n",
    "        optimize_model(optimizer, policy_net, target_net, eps_greedy_memory)\n",
    "\n",
    "    if (i_episode + 1) % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "    if (i_episode + 1) % 100 == 0:\n",
    "        print('Episode ' + str(i_episode))\n",
    "        print('Cumulative reward: ' + str(cumulative_rewards[i_episode]))\n",
    "        \n",
    "print('Complete')\n",
    "plt.title('Training...')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Duration')\n",
    "plt.plot(cumulative_rewards_2)\n",
    "plt.show()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 99\n",
      "episode 199\n",
      "episode 299\n",
      "episode 399\n",
      "episode 499\n",
      "episode 599\n",
      "episode 699\n",
      "episode 799\n",
      "episode 899\n",
      "episode 999\n",
      "Complete\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "K = 4\n",
    "\n",
    "policy_nets = [DQN(INPUT_SIZE, OUTPUT_SIZE).to(device) for _ in range(K)]\n",
    "target_nets = [DQN(INPUT_SIZE, OUTPUT_SIZE).to(device) for _ in range(K)]\n",
    "memories = [ReplayMemory(100000) for _ in range(K)]\n",
    "for k in range(K):\n",
    "    target_nets[k].load_state_dict(policy_nets[k].state_dict())\n",
    "    target_nets[k].eval()  \n",
    "\n",
    "optimizers = [optim.RMSprop(policy_nets[k].parameters()) for k in range(K)]\n",
    "num_episodes = 1000\n",
    "episode_steps = []\n",
    "for i_episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    state = torch.tensor([state], device=device, dtype=torch.float)\n",
    "    for t in count():\n",
    "        cur_net_idx = np.random.choice(K, 1)[0]\n",
    "        policy_net = policy_nets[cur_net_idx]\n",
    "        \n",
    "        action = select_action(policy_net, state)\n",
    "#         if i_episode > 990:\n",
    "#             env.render()\n",
    "        next_state, reward, done, _ = env.step(action.item())\n",
    "        next_state = torch.tensor([next_state], device=device, dtype=torch.float)\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        if done:\n",
    "            next_state = None\n",
    "            \n",
    "        for k in range(K):\n",
    "            noise = np.random.normal(0, 0.1)\n",
    "            memories[k].push(state, action, next_state, reward + noise)\n",
    "            if np.random.choice(2, 1)[0] == 1:\n",
    "                memories[k].push(state, action, next_state, reward)\n",
    "            optimize_model(optimizers[k], policy_nets[k], target_nets[k], memories[k])\n",
    "        \n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            episode_steps.append(t + 1)\n",
    "            break\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        for k in range(K):\n",
    "            target_nets[k].load_state_dict(policy_nets[k].state_dict())\n",
    "        \n",
    "    if (i_episode + 1) % 100 == 0:\n",
    "        print('episode ' + str(i_episode))\n",
    "\n",
    "print('Complete')\n",
    "plt.title('Training...')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Duration')\n",
    "plt.plot(episode_steps)\n",
    "plt.show()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
